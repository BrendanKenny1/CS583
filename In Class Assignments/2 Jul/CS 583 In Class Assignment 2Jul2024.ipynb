{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c47f06",
   "metadata": {},
   "source": [
    "# Stevens Institute of Technology\n",
    "# CS 583: NN\n",
    "\n",
    "# Names: Brendan Kenny, Kimberly Maldonado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c53612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1f1b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4904 - loss: 0.7078 - val_accuracy: 0.4875 - val_loss: 0.7003\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5365 - loss: 0.6798 - val_accuracy: 0.5188 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5784 - loss: 0.6699 - val_accuracy: 0.5375 - val_loss: 0.6882\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6549 - loss: 0.6503 - val_accuracy: 0.5562 - val_loss: 0.6840\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6305 - loss: 0.6489 - val_accuracy: 0.5688 - val_loss: 0.6760\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6585 - loss: 0.6278 - val_accuracy: 0.6062 - val_loss: 0.6661\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7123 - loss: 0.5984 - val_accuracy: 0.6187 - val_loss: 0.6573\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7492 - loss: 0.5942 - val_accuracy: 0.6438 - val_loss: 0.6457\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7785 - loss: 0.5706 - val_accuracy: 0.6562 - val_loss: 0.6333\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7825 - loss: 0.5490 - val_accuracy: 0.6812 - val_loss: 0.6207\n",
      "Original Model - Train Accuracy: 0.7862, Test Accuracy: 0.7150\n",
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.4916 - loss: 0.6942 - val_accuracy: 0.4750 - val_loss: 0.6853\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5283 - loss: 0.6713 - val_accuracy: 0.6062 - val_loss: 0.6674\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7010 - loss: 0.6471 - val_accuracy: 0.6000 - val_loss: 0.6467\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - loss: 0.5801 - val_accuracy: 0.6250 - val_loss: 0.6281\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7836 - loss: 0.5100 - val_accuracy: 0.7063 - val_loss: 0.5834\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7934 - loss: 0.4431 - val_accuracy: 0.7625 - val_loss: 0.4696\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8500 - loss: 0.3591 - val_accuracy: 0.7875 - val_loss: 0.4339\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8792 - loss: 0.2768 - val_accuracy: 0.8062 - val_loss: 0.3646\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9262 - loss: 0.2127 - val_accuracy: 0.8375 - val_loss: 0.3641\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9051 - loss: 0.2357 - val_accuracy: 0.8250 - val_loss: 0.3766\n",
      "Deeper Model - Train Accuracy: 0.9137, Test Accuracy: 0.8349999785423279\n",
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.5016 - loss: 0.6933 - val_accuracy: 0.4812 - val_loss: 0.6937\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5074 - loss: 0.6931 - val_accuracy: 0.4812 - val_loss: 0.6936\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4742 - loss: 0.6937 - val_accuracy: 0.4812 - val_loss: 0.6934\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4935 - loss: 0.6933 - val_accuracy: 0.4812 - val_loss: 0.6933\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5171 - loss: 0.6931 - val_accuracy: 0.4812 - val_loss: 0.6932\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4852 - loss: 0.6932 - val_accuracy: 0.5188 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4790 - loss: 0.6932 - val_accuracy: 0.5188 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4868 - loss: 0.6932 - val_accuracy: 0.5188 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4845 - loss: 0.6932 - val_accuracy: 0.5188 - val_loss: 0.6930\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5199 - loss: 0.6930 - val_accuracy: 0.5188 - val_loss: 0.6929\n",
      "Deepest Model - Train Accuracy: 0.5075, Test Accuracy: 0.48500001430511475\n"
     ]
    }
   ],
   "source": [
    "# Generate fake data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 20)\n",
    "y = (np.sum(X, axis=1) > 10).astype(int)\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define the original neural network\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(20,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# Train the original model\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "5\n",
    "# Evaluate the original model\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Original Model - Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "# Modify the model by adding more layers\n",
    "def create_deeper_model():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(20,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# Train the modified model\n",
    "deeper_model = create_deeper_model()\n",
    "deeper_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Evaluate the modified model\n",
    "deeper_train_loss, deeper_train_acc = deeper_model.evaluate(X_train, y_train, verbose=0)\n",
    "deeper_test_loss, deeper_test_acc = deeper_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Deeper Model - Train Accuracy: {deeper_train_acc:.4f}, Test Accuracy: {deeper_test_acc}\")\n",
    "def create_deepest_model():    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(20,)),\n",
    "        Dense(64, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(16, activation='softmax'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# Train the modified model\n",
    "deepest_model = create_deepest_model()\n",
    "deepest_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Evaluate the modified model\n",
    "deepest_train_loss, deepest_train_acc = deepest_model.evaluate(X_train, y_train, verbose=0)\n",
    "deepest_test_loss, deepest_test_acc = deepest_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Deepest Model - Train Accuracy: {deepest_train_acc:.4f}, Test Accuracy: {deepest_test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25cc98",
   "metadata": {},
   "source": [
    "## How did the training and test accuracies change with the addition of more layers?\n",
    "\n",
    "The training accuracies improved with the addition of more layers. The test accuracies increased but not as significantly as the training accuracy did. This is due to overfitting within the model.  \n",
    "\n",
    "When testing the deepest model with the softmax activation function both the train and test accuracy declined significantly. This is due to the vanishing gradient problem that is resolved with the use of the ReLu, or rectified linear unit, activation function.\n",
    "\n",
    "## Did the deeper network show signs of overfitting or underfitting? Why do you think that is?\n",
    "\n",
    "The model showed some overfitting as it got significantly deeper and the training accuracy continues to increase while the test accuracy did not.  \n",
    "\n",
    "## What strategies could be employed to improve the performance of the deeper network?\n",
    "\n",
    "Adding in early stopping is a way to prevent overfitting of the model because it will stop training once the improvements on the training set become nominal. Another method would be adding dropout, a regularization technique where some neurons are randomly dropped during training, is another way to prevent the model from overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d09ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
