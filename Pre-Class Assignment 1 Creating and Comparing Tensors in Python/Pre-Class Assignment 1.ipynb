{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4]\n",
      " [5 6]]\n",
      "NP tensor foolowing the addition: [[ 7  8]\n",
      " [ 9 10]]\n",
      "NP tensor foolowing the multiplication: [[15 20]\n",
      " [25 30]]\n",
      "NP tensor foolowing the reshape: [[3]\n",
      " [4]\n",
      " [5]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "#creating \"tensor\" array using Numpy\n",
    "numpy_tensor = np.array([[3,4],[5,6]])\n",
    "#printing array\n",
    "print(numpy_tensor)\n",
    "#applying operations (addition/multiplication/reshape) and printing results \n",
    "np_tensor_add = numpy_tensor + 4\n",
    "np_tensor_multiply = numpy_tensor * 5\n",
    "np_tensor_reshape = numpy_tensor.reshape(4,1)\n",
    "print('NP tensor foolowing the addition:', np_tensor_add)\n",
    "print('NP tensor foolowing the multiplication:', np_tensor_multiply)\n",
    "print('NP tensor foolowing the reshape:', np_tensor_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 7  8]\n",
      " [ 9 10]], shape=(2, 2), dtype=int32)\n",
      "TF tensor foolowing the addition: tf.Tensor(\n",
      "[[11 12]\n",
      " [13 14]], shape=(2, 2), dtype=int32)\n",
      "TF tensor foolowing the multiplication: tf.Tensor(\n",
      "[[35 40]\n",
      " [45 50]], shape=(2, 2), dtype=int32)\n",
      "TF tensor foolowing the reshape: tf.Tensor(\n",
      "[[ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]], shape=(4, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#creating tensor using TensorFlow\n",
    "tf_tensor = tf.constant([[7,8],[9,10]])\n",
    "#printing tensor\n",
    "print(tf_tensor)\n",
    "#applying operations (addition/multiplication/reshape) and printing results\n",
    "tf_tensor_add = tf_tensor + 4\n",
    "tf_tensor_multiply = tf_tensor * 5\n",
    "tf_tensor_reshape = tf.reshape(tf_tensor, [4,1])\n",
    "print('TF tensor foolowing the addition:', tf_tensor_add)\n",
    "print('TF tensor foolowing the multiplication:', tf_tensor_multiply)\n",
    "print('TF tensor foolowing the reshape:', tf_tensor_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x (NumPy): 6.0\n",
      "Gradient of y with respect to x (TensorFlow): tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#TensorFlow GradientTape function allows the calculation of a gradients, which is useful for backpropagation in neural networks. With NumPy the gradient can be found by implementing it manually.\n",
    "\n",
    "x_np = np.array(3.0)\n",
    "y_np = x_np * x_np\n",
    "# Compute the gradient manually\n",
    "dy_dx_np = 2 * x_np\n",
    "print(\"Gradient of y with respect to x (NumPy):\", dy_dx_np)\n",
    "\n",
    "#computing gradient with TensorFlow GradientTape \n",
    "x_tf = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    y_tf = x_tf * x_tf\n",
    "\n",
    "dy_dx_tf = tape.gradient(y_tf, x_tf)\n",
    "print(\"Gradient of y with respect to x (TensorFlow):\", dy_dx_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution with regular tensor: [9]\n",
      "Eager execution with TensorFlow tensor: tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "TensorFlow without eager execution: [9.]\n"
     ]
    }
   ],
   "source": [
    "#Eager execution allows for operations to be executed immediatly which is very convenient. This is how NumPy arrays typically operate. But TensorFlow also has the option to turn off Eager execution which NumPy does not. \n",
    "\n",
    "#attempting eager execution with Python lists results in an error because python does not allow the multiplication of two lists but it does work with NumPy.\n",
    "x2 = np.array([3])\n",
    "\n",
    "x = [3.0]\n",
    "try:\n",
    "    y = x2 * x2\n",
    "    print(\"Eager execution with regular tensor:\", y)\n",
    "except Exception as e:\n",
    "    print(\"Error with regular tensor:\", e)\n",
    "\n",
    "#with Eager execution enabled the tensor can be multiplied by itself resulting in the operation product\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "y = x * x\n",
    "print(\"Eager execution with TensorFlow tensor:\", y)\n",
    "\n",
    "#with Eager execution turned off the operations can only be completed with in a session, after TensorFlow builds a computational graph\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "x = tf.constant([3.0])\n",
    "y = x * x\n",
    "\n",
    "#creating session to run operations\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    result = sess.run(y)\n",
    "    print(\"TensorFlow without eager execution:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Ragged Tensor addition:\n",
      "<tf.RaggedTensor [[11, 22, 33], [44, 55], [66]]>\n",
      "Python Nested Lists addition:\n",
      "[[11, 22, 33], [44, 55], [66]]\n"
     ]
    }
   ],
   "source": [
    "#TensorFlow Ragged Tensor allows the creation of tensors that can represent irregular shapes. Something similar can be achieved with nested lists but these do not allow operations to be performed.\n",
    "#TensorFlow Ragged Tensor creation\n",
    "rt1 = tf.ragged.constant([[1, 2, 3], [4, 5], [6]])\n",
    "rt2 = tf.ragged.constant([[10, 20, 30], [40, 50], [60]])\n",
    "\n",
    "#performing element-wise addition\n",
    "rt_sum = rt1 + rt2\n",
    "print(\"TensorFlow Ragged Tensor addition:\")\n",
    "print(rt_sum)\n",
    "\n",
    "#Ragged lists made using python nested lists\n",
    "ragged_list1 = [[1, 2, 3], [4, 5], [6]]\n",
    "ragged_list2 = [[10, 20, 30], [40, 50], [60]]\n",
    "\n",
    "#performing element-wise addition manually\n",
    "ragged_list_sum = [[a + b for a, b in zip(sublist1, sublist2)] \n",
    "                   for sublist1, sublist2 in zip(ragged_list1, ragged_list2)]\n",
    "\n",
    "print(\"Python Nested Lists addition:\")\n",
    "print(ragged_list_sum)\n",
    "\n",
    "#manipulation with Ragged Tensor is far more convenient and easier to implement than doing so with pythons built in nested lists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Comparison and Explanation\n",
    "\n",
    "On the very surface regular NumPy arrays and TensorFlow tensors are quite similar. That being said there are some key differences that make TensorFlow tensors more suitable for complex computations, especially in machine learning and more specifically deep learning.\n",
    "\n",
    "A NumPy array and TensorFlow tensor are similar in that they both can store and manipulate multidimensional data. However, TensorFlow tensors are designed for building computational graphs, which is an abstraction defined by the flow of operations on the data. These computational graphs allow for more complex and multi-step compuations. NumPy arrays are used more generally for numerical computations.\n",
    "\n",
    "TensorFlow tensors can enable the use of GPUs (graphics processing units) for faster computations compared to NumPy arrays, which are typically CPU-bound. The TensorFlow tensor is stored in the GPU memory while the NumPy array is stored in the local memory of the computer. This makes tensors more powerful when applied in deep learning because larger and deeper networks can be trainer quicker through GPU operations than standard NumPy arrays.\n",
    "\n",
    "In general, TensorFlow tensors offer a quicker and more flexible framework for complex numerical computations, which are needed when training machine learning algorithms on a large amount of data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
